# AI Sales Agent - Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Choose which provider to use for each capability
# Options: minimax, vllm (for text), coqui (for TTS), sadtalker (for video)

PROVIDER_TEXT=minimax
PROVIDER_TTS=minimax
PROVIDER_VIDEO=minimax

# Fallback chains (comma-separated, tried in order)
PROVIDER_TEXT_FALLBACK=vllm,minimax
PROVIDER_TTS_FALLBACK=coqui,minimax
PROVIDER_VIDEO_FALLBACK=sadtalker,minimax

# =============================================================================
# MINIMAX (Default Cloud Provider)
# =============================================================================
# Get API key from: https://www.minimax.io/
# Get Group ID from: https://www.minimax.io/platform/user-center/basic-information

MINIMAX_API_KEY=your-api-key-here
MINIMAX_GROUP_ID=your-group-id-here
MINIMAX_BASE_URL=https://api.minimax.io/v1

# =============================================================================
# vLLM (Self-hosted Text Generation)
# =============================================================================
# Start vLLM server with:
#   python -m vllm.entrypoints.openai.api_server \
#     --model meta-llama/Llama-3.1-70B-Instruct

PROVIDER_VLLM_BASE_URL=http://localhost:8000
PROVIDER_VLLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
# PROVIDER_VLLM_API_KEY=  # Optional, if vLLM requires auth

# =============================================================================
# COQUI XTTS (Self-hosted TTS)
# =============================================================================
# Install with: pip install TTS>=0.22.0
# Model will be downloaded automatically on first use

PROVIDER_COQUI_DEVICE=cuda
PROVIDER_COQUI_MODEL=tts_models/multilingual/multi-dataset/xtts_v2
PROVIDER_COQUI_VOICES_DIR=./voices

# =============================================================================
# SADTALKER (Self-hosted Talking Head Video)
# =============================================================================
# Clone from: https://github.com/OpenTalker/SadTalker
# Download checkpoints from the repo

PROVIDER_SADTALKER_CHECKPOINT=./models/sadtalker
PROVIDER_SADTALKER_DEVICE=cuda
PROVIDER_SADTALKER_PREPROCESS=crop
PROVIDER_SADTALKER_STILL=false
# PROVIDER_SADTALKER_ENHANCER=gfpgan  # Optional: gfpgan or RestoreFormer

# =============================================================================
# PIPELINE SETTINGS
# =============================================================================
ENABLE_PERSONALIZED_PIPELINE=true

# =============================================================================
# GPU REQUIREMENTS (DGX SPARK)
# =============================================================================
# vLLM (Llama 70B): ~40GB VRAM
# XTTS v2: ~6GB VRAM
# SadTalker: ~8GB VRAM
# Total: ~54GB (fits on A100 80GB or DGX)
